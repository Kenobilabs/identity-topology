#default remote
# keep run location remote, if you are running on any cluster
run.location = remote

source.type = kafka

#source Kakfa Point
sourceZooKeeperURL=192.168.70.101:2181
sourceBrokerId=192.168.70.101:9092
inputTopic = source-cloud-string
#Kafka consumer fetch size 
# default 1048576 , dont exceed
kafka.consumer.fetch.size.byte=1048576
#Kafka consumer buffer size 
# default 1048576 , dont exceed
kafka.consumer.buffer.size.byte=1048576
#Kafka consumer default buffer size

#SSL
spout.ssl.truststore.location=/var/kafkaSsl/clientKeys/truststore.jks
spout.ssl.truststore.password=test

#keystore

spout.ssl.keystore.location=
spout.ssl.keystore.password=
spout.ssl.key.password=

#destination
#rabbit or kafka
destination.type=rabbit
#default 60
rabbitmq.heartbeat=30

#destination RabbitMq points   
output.rabbitmq.queueName0=file_sha2.classify
output.rabbitmq.queue.durable0=true

output.rabbitmq.host0=10.173.244.90
output.rabbitmq.ha.hosts0=10.173.244.90
output.rabbitmq.port0=5672

output.rabbitmq.username0=guest
output.rabbitmq.password0=guest
output.rabbitmq.routingKey0=

output.rabbitmq.virtualhost0=shasta
output.topology.spout.max.batch.size0=100


# any standard name, dont keep empty
output.rabbitmq.exchange.name0=shasta
# four types direct topic fanout headers , have only tested direct and topic
output.rabbitmq.exchange.type0=direct
output.rabbitmq.exchange.durable0=true
output.rabbitmq.exchange.exclusive0=false

# application/json  or application/octet-stream
output.rabbitmq.contentType0=application/json
# dont encode for bytes/string unless needed, have tested once for "UTF-8", generally not needed
output.rabbitmq.encoding0=

 
         
# Parallelism, Dont modify until complete understanding
# no of nodes
topology.workers=1
# no of executors
spoutParallelCount=1
boltParallelCount=1
# metrics
metricsParallelCount=1

# Rate Limiting / Sampling
# 'Yes'  for enabling rateLimiting or 'No' to disable
rate.limit=No

# Choose between the types, 'percent' for specifying Percentage of allowed messages/tuples for every 100 count , 'value' for specifying tuples/messages per minute
# percent or value
rate.type=value
# Percentage Count , between 0 and 100 no decimals
percent.limit=40
# ValueCount , messages/tuples per minute (max value allowed is LONG.MAX)
value.limit=20

#encoding  kafka.serializer.DefaultEncoder for bytes and kafka.serializer.StringEncoder for String 
serializerEncodingValue = kafka.serializer.StringSchema
# Field Name , bytes for bytes , str for StringSchema
partitionFieldName = str

# raw for RawScheme (bytes) , string for StringScheme
schemeType = string
 

 

#Update# keep topologyName and StreamName unique everytime.

topology.name = identity-v3-kafka-rabbit3
streamName = identity-v3-kafka-rabbit3
 
 
#Mostly static values # keep topology.name and StreamName unique everytime.
topology.message.timeout.secs=300
topology.max.spout.pending=1
message.send.max.retries=0

#statsD
# yes or no
statsd=yes
metrics.statsd.host=localhost
metrics.statsd.port=8125
# default  storm.metrics
metrics.statsd.prefix=cpe